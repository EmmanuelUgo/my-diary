[
  {
    "path": "posts/2022-02-18-chatanalyzer-part-1/",
    "title": "Building a Chat Analyzer app - Part 2",
    "description": "Creating some descriptive information about our data",
    "author": [
      {
        "name": "Emmanuel Ugochukwu",
        "url": "https://github.com/emmanuelugo"
      }
    ],
    "date": "2022-02-18",
    "categories": [
      "Text Analytics",
      "Data Science",
      "WhatsApp series"
    ],
    "contents": "\r\n\r\nContents\r\nLoading Libraries and Data\r\nAnalysis\r\nQuestion 1\r\nQuestion 2\r\nQuestion 3\r\nQuestion 4\r\nQuestion 5\r\nQuestion 6\r\nQuestion 7\r\n\r\n\r\nHello and welcome back.\r\nIn this post, I go over some of the logic that went on the first page of the WhatsApp chat analyzer. From my previous post, this section was categorized under Chat Summary Feature. This post will cover the cards section. Most of the analysis conducted here would eventually be converted to functions so that we don‚Äôt have a lot of code in the shiny server.\r\nLet‚Äôs dig in!\r\nLoading Libraries and Data\r\nFor demonstration purposes, I‚Äôll be using a group chat data. It‚Äôs a small group chat where some of my good friends in school and I talk about things. I have already exported the data to my PC so I‚Äôd just load the necessary libraries and data and do some basic filtering and data transformations.\r\n\r\n\r\nlibrary(tidyverse) ## Modern Data science library in R\r\nlibrary(rwhatsapp) ## Smart way to load whatsapp data in R\r\n\r\nchat_data <- rwa_read(\"Chat de WhatsApp con The Peng Geng.txt\") %>%\r\n  filter(!is.na(author), !text %in% c(\"Se eliminÊòº„∏≥ este mensaje\")) %>%\r\n  mutate(\r\n    time = as.Date(time),\r\n    author = as.character(author),\r\n    author = ifelse(author == \"Da Airtel\", \"haanuel_NG\", author)\r\n  ) %>%\r\n  select(-source)\r\n\r\nchat_data %>%\r\n  slice_head(n = 5)\r\n\r\n\r\n# A tibble: 5 x 5\r\n  time       author     text                          emoji emoji_name\r\n  <date>     <chr>      <chr>                         <lis> <list>    \r\n1 2021-05-01 Exon       \"Noel what‚Äôs good\"            <NUL~ <NULL>    \r\n2 2021-05-01 Exon       \"Going well bro. Good to hav~ <NUL~ <NULL>    \r\n3 2021-05-01 Exon       \"My internet gave out.\"       <NUL~ <NULL>    \r\n4 2021-05-01 Anthony EE \"What now happened\"           <NUL~ <NULL>    \r\n5 2021-05-01 Anthony EE \"Ah. Where can I send my acc~ <NUL~ <NULL>    \r\n\r\nThe rwa_read() function loads the chat data in R\r\nThe filter() function removes global messages like ‚ÄúSe te a√±adi√≥ al grupo‚Äù meaning you were added to the group. Basically, messages that weren‚Äôt sent by any user, just whatsapp noticifications. The second thing the filter() does is to remove deleted messages. ‚ÄúSe elimin√≥ este mensaje‚Äù meaning, ‚Äúthis message was deleted‚Äù appears when a user deletes their messages.\r\nThe mutate() function converts the datetime column to a date column.\r\nThe select() function removes the source column, we wouldn‚Äôt be using that. Finally, our chat data is ready for analysis.\r\nNOTE: My phone is in spanish, ‚ÄúSe elimin√≥ este mensaje‚Äù would not work if your phone is set to a different language. One way to go around this is to load the data without any data wrangling steps, then take a look at the data to know the tags used.\r\nAnalysis\r\nThe question I wanted to explore are as follows, feel free to go crazy here.\r\nTotal number of chats sent\r\nTotal number of users that has sent a message\r\nWho sends the most messages?\r\nWhat is the average word usage per chat and who has the highest?\r\nWho chats less?\r\nWho uses emojis üòä the most?\r\nWhat is the most used emoji?\r\nHow many times has the user with the most emoji usage used the most used emoji?\r\nWho sends Multimedia files the most?\r\nFor all messages sent by a user, how many emojis was used and who is the highest on this stat?\r\nI‚Äôll walk you through on my thought process for the first 7 questions. Feel free to try out the last three yourself and also the questions never really end. Go wild!\r\nAs Julia Silge would say, Alright!!\r\nQuestion 1\r\nTo get the total number of chat sent, we just count the number of rows in the tibble. Since we have done the basic data cleaning in previous steps, the data only shows chats. The prettyNum() function simply formats the numbers. i.e, 2000 = 2,000.\r\n\r\n\r\ntt_mesages <- function(tbl) {\r\n  prettyNum(nrow(tbl), big.mark = \",\")\r\n}\r\n\r\ntt_mesages(chat_data)\r\n\r\n\r\n[1] \"7,117\"\r\n\r\nQuestion 2\r\nThe logic for this is quite simple.\r\nGet the distinct users (It would give a vector of characters)\r\nGet the length of the new result, that would give the number of users in that chat.\r\n\r\n\r\ntt_users <- function(tbl) {\r\n  prettyNum(length(unique(tbl$author)), big.mark = \",\")\r\n}\r\n\r\ntt_users(chat_data)\r\n\r\n\r\n[1] \"7\"\r\n\r\nNOTE: Using the tidyverse approach would have been a cleaner approach but using base R functions in shiny has some benefits. This is how you can approach this problem using the tidy approach.\r\n\r\n\r\nchat_data %>%\r\n  distinct(author) %>%\r\n  summarize(users = n()) %>%\r\n  pull(users)\r\n\r\n\r\n[1] 7\r\n\r\nQuestion 3\r\nTo know who sends the most messages, we just count the number of messages sent by each user and select the highest score.\r\n\r\n\r\ntt_active_user <- function(tbl) {\r\n  tbl %>%\r\n    count(author) %>%\r\n    slice_max(order_by = n, n = 1, with_ties = FALSE) %>%\r\n    mutate(\r\n      author = as.character(author) %>%\r\n        str_wrap(width = 10),\r\n      n = prettyNum(n, big.mark = \",\")\r\n    )\r\n}\r\n\r\ntt_active_user(chat_data)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  author     n    \r\n  <chr>      <chr>\r\n1 haanuel_NG 2,204\r\n\r\nNOTE: the with_ties = FALSE argument in the slice_max() function helps so that we only have one user. For example, if two users have the same amount of messages but we only need one, this argument would only pick the first person on the tibble but setting it to true would return the two users.\r\nIt seems I have sent the most messages üòÇ\r\nQuestion 4\r\nTo know who uses more words in a single message, I broke the logic into two parts. First, I took a random sample of 500 chats sent by each user then did the average count of words used by each user and selected the user with the highest score. After that, I ran a simulation to repeat the same process 20 times in a way to eliminate bias on a particular sample.\r\nThe tt_highest_avg_length() function works like this.\r\nAccepts a tibble of chat data as input.\r\nRemoves non-text chats (multimedia files/tags)\r\nSamples 500 chats each from every user and gets the number of characters used in every chat.\r\nSummarizes the data by getting the total number of messages sent and the average number of words used.\r\nRemoves records for users that hasn‚Äôt sent up to 500 messages.\r\nFinally, it returns the highest record.\r\nAs we can see from the result below, my friend Gerald sends an average of ~10 words per message making him top in this metric.\r\n\r\n\r\ntt_highest_avg_length <- function(tbl) {\r\n  tbl %>%\r\n    filter(!str_detect(text, \"^<\")) %>%\r\n    select(author, text) %>%\r\n    nest(data = c(text)) %>%\r\n    mutate(sample_chats = map(data, .f = function(tbl) slice_sample(tbl, n = 500))) %>%\r\n    select(author, sample_chats) %>%\r\n    unnest(sample_chats) %>%\r\n    mutate(text_length = str_count(text, \"\\\\w+\")) %>%\r\n    filter(!is.na(text_length)) %>%\r\n    group_by(author) %>%\r\n    summarize(\r\n      chat_freq = n(),\r\n      avg_text = mean(text_length), .groups = \"drop\"\r\n    ) %>%\r\n    filter(chat_freq == 500) %>%\r\n    slice_max(order_by = avg_text, n = 1, with_ties = FALSE) %>%\r\n    mutate(author = as.character(author) %>%\r\n      str_wrap(width = 10)) %>%\r\n    select(-chat_freq)\r\n}\r\n\r\n\r\ntt_highest_avg_length(chat_data) %>%\r\n  bind_rows(tt_highest_avg_length(chat_data)) %>%\r\n  bind_rows(tt_highest_avg_length(chat_data)) %>%\r\n  bind_rows(tt_highest_avg_length(chat_data)) %>%\r\n  bind_rows(tt_highest_avg_length(chat_data))\r\n\r\n\r\n# A tibble: 5 x 2\r\n  author   avg_text\r\n  <chr>       <dbl>\r\n1 Gerald..     9.94\r\n2 Gerald..    10.6 \r\n3 Gerald..    10.6 \r\n4 Gerald..    10.5 \r\n5 Gerald..    10.8 \r\n\r\nSampling only once might not be enough, there might be some bias in our results. From the results above, Gerald appeared to have the highest score all five times, but it could be different for another data. The method I chose to solve this issue is to run a simulation and pick the highest score afterwards as the final result.\r\nThe tt_get_avg_chatter() function works like this:\r\nTakes the chat data as input and creates 20 instances of it.\r\nFor each instance of the data, it applies the tt_highest_avg_length() function to it.\r\nThis would create 20 different results for the ‚Äòauthor‚Äô and ‚Äòavg_text‚Äô.\r\nIt wraps up by getting the average word length for each of the authors and returns the highest score and its final output after rounding up to an integer.\r\n\r\n\r\ntt_get_avg_chatter <- function(tbl) {\r\n  tibble(n = 1:20) %>%\r\n    mutate(data = list(tbl)) %>%\r\n    mutate(sim = map(data, tt_highest_avg_length)) %>%\r\n    select(sim) %>%\r\n    unnest(sim) %>%\r\n    count(author, wt = mean(avg_text)) %>%\r\n    slice_max(order_by = n, n = 1, with_ties = F) %>%\r\n    mutate(n = round(n, 0))\r\n}\r\n\r\ntt_get_avg_chatter(chat_data)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  author       n\r\n  <chr>    <dbl>\r\n1 Gerald..    10\r\n\r\nNOTE: Even if the simulation returns the same user 20 times. The summarize function would just get the average of those 20 simulated results.\r\nQuestion 5\r\nTo know who chats less, we basically repeat the procedure for Question 3 only this time, we pick the lowest count.\r\n\r\n\r\ntt_least_active_user <- function(tbl) {\r\n  tbl %>%\r\n    count(author) %>%\r\n    slice_min(order_by = n, n = 1, with_ties = FALSE) %>%\r\n    mutate(\r\n      author = as.character(author) %>%\r\n        str_wrap(width = 10),\r\n      n = prettyNum(n, big.mark = \",\")\r\n    )\r\n}\r\n\r\ntt_least_active_user(chat_data)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  author n    \r\n  <chr>  <chr>\r\n1 Exon   281  \r\n\r\nQuestion 6\r\nTo know who uses emojis the most, we have to unnest the emoji column of the dataset. This column has a list of all emojis used in a chat. Upon unnesting this column, we simply count the number of times each user used an emoji and select the person with the highest number as the person who uses emojis the most.\r\n\r\n\r\ntt_who_uses_emoji_mostly <- function(tbl) {\r\n  tbl %>%\r\n    select(author, emoji) %>%\r\n    unnest(emoji) %>%\r\n    filter(!is.na(emoji)) %>%\r\n    count(author) %>%\r\n    slice_max(order_by = n, n = 1, with_ties = F) %>%\r\n    mutate(\r\n      author = as.character(author),\r\n      n = prettyNum(n, big.mark = \",\")\r\n    )\r\n}\r\n\r\ntt_who_uses_emoji_mostly(chat_data)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  author n    \r\n  <chr>  <chr>\r\n1 NFF    796  \r\n\r\nQuestion 7\r\nKnowing which emoji was used the most is quite similar to some of the previous questions asked. Simply get the emoji data, count it and select the highest number.\r\n\r\n\r\ntt_extract_most_used_emoji <- function(tbl) {\r\n  chat_data %>%\r\n    select(emoji) %>%\r\n    unnest(emoji) %>%\r\n    filter(!is.na(emoji)) %>%\r\n    count(emoji, sort = TRUE) %>%\r\n    slice_max(order_by = n, n = 1, with_ties = F) %>%\r\n    mutate(n = prettyNum(n, big.mark = \",\"))\r\n}\r\n\r\n\r\n\r\nResult: üòÇ has been used 1,280 times..\r\nThank you so much for getting up to this point, I know this post was quite long and I hope you gained some valuable content from this.\r\nIn the next post, I‚Äôll walk you through on how I coupled all these functions into Shiny.\r\nSee you in the next one!\r\n‚ù§Ô∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-18-chatanalyzer-part-1/preview.png",
    "last_modified": "2022-02-20T11:33:39+01:00",
    "input_file": "chatanalyzer-part-1.knit.md",
    "preview_width": 326,
    "preview_height": 259
  },
  {
    "path": "posts/2022-02-10-vroom-vs-readr/",
    "title": "Vroom vs Readr",
    "description": "A faster way to load data in R",
    "author": [
      {
        "name": "Emmanuel Ugochukwu",
        "url": "https://github.com/emmanuelugo"
      }
    ],
    "date": "2022-02-10",
    "categories": [
      "R Packages",
      "readr",
      "vroom"
    ],
    "contents": "\r\n\r\nContents\r\nLoading Libraries\r\nCreating a tracking function\r\nVisualization\r\nConclusion\r\n\r\nThe {vroom} package has been out for a while now and in this post, we will be comparing it to the {readr} package. Both packages are used to load data in R. Today, we‚Äôll be looking at the time it takes vroom() from the vroom package to load data and comparing it to the read_csv() function from the {readr} package. The {readr} package is part of the {tidyverse} ecosystem.\r\nWho knows, you might learn something from my codes ü§™\r\nLoading Libraries\r\n\r\n\r\nlibrary(vroom)\r\nlibrary(tidyverse)\r\nlibrary(ggimage)\r\nlibrary(ggtext)\r\nlibrary(showtext)\r\n\r\n\r\nfont_add_google(name = \"quicksand\", family = \"quicksand\")\r\n\r\n\r\n\r\nCreating a tracking function\r\nThis function takes a single argument, path (location to the file on your PC) and records the time it took to load that file using the vroom() and read_csv() function and return a tibble of results with the size of the data.\r\n\r\n\r\nget_time_diff <- function(path) {\r\n  ## For Vroom\r\n  start_vroom <- Sys.time()\r\n\r\n  vroom_data <- vroom(path, show_col_types = FALSE)\r\n\r\n  end_vroom <- Sys.time()\r\n\r\n  result_vroom <- end_vroom - start_vroom\r\n\r\n  ## For readR\r\n  start_readr <- Sys.time()\r\n\r\n  readr_data <- read_csv(path)\r\n\r\n  end_readr <- Sys.time()\r\n\r\n  result_readr <- end_readr - start_readr\r\n\r\n  final_table <- tribble(\r\n    ~vroom, ~readr, ~size,\r\n    ##########################\r\n    result_vroom, result_readr, format(object.size(vroom_data), units = \"Gb\")\r\n  )\r\n\r\n  return(final_table)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nresults <- get_time_diff(paths) %>%\r\n  select(size, readr, vroom) %>%\r\n  mutate_at(c(\"readr\", \"vroom\"), as.numeric)\r\n\r\n\r\n\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#xzhtvywnsm .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#xzhtvywnsm .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#xzhtvywnsm .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#xzhtvywnsm .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#xzhtvywnsm .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#xzhtvywnsm .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#xzhtvywnsm .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#xzhtvywnsm .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#xzhtvywnsm .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#xzhtvywnsm .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#xzhtvywnsm .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#xzhtvywnsm .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#xzhtvywnsm .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xzhtvywnsm .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#xzhtvywnsm .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#xzhtvywnsm .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#xzhtvywnsm .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#xzhtvywnsm .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#xzhtvywnsm .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#xzhtvywnsm .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#xzhtvywnsm .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#xzhtvywnsm .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\n\r\n      size\r\n      readr\r\n      vroom\r\n    1\r\n1.4 Gb\r\n47.80 Sec\r\n11.47 Sec\r\n\r\nVisualization\r\n\r\n\r\nresults <- results %>%\r\n  pivot_longer(cols = c(vroom, readr))\r\n\r\nsticker_pos <- tibble(\r\n  x = c(\"vroom\", \"readr\"),\r\n  y = c(results$value[1] / 2, results$value[2] / 2),\r\n  sticker = c(\"vroom.png\", \"readr.png\")\r\n)\r\n\r\n\r\n\r\n\r\n\r\nggplot() +\r\n  geom_col(data = results, aes(name, value)) +\r\n  geom_hline(yintercept = results$value[2] / 2, lty = 2) +\r\n  geom_image(data = sticker_pos, aes(x, y, image = sticker), size = 0.15) +\r\n  annotate(geom = \"text\", x = \"vroom\", y = (results$value[2] / 2) + 2, label = \"Half time mark for readr\") +\r\n  expand_limits(y = c(0, 50)) +\r\n  labs(\r\n    title = \"**{vroom}** vs **{readr}**\",\r\n    subtitle = \"Comparing the time it takes to load a data size of 1.4Gb\",\r\n    y = \"Time (Sec)\",\r\n    x = NULL\r\n  ) +\r\n  theme_minimal() +\r\n  theme(\r\n    text = element_text(family = \"quicksand\"),\r\n    plot.title = element_markdown()\r\n  )\r\n\r\n\r\n\r\n\r\nConclusion\r\nFrom the plot above, the vroom package did way better than the readr package, reading a 1.4Gb csv file far lower than half the time it took readr to load. The vroom package is an excellent way to load large data sizes in R.\r\nNow you know a new and fast way to load data in R.\r\n‚ù§Ô∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-vroom-vs-readr/distill-preview.png",
    "last_modified": "2022-02-12T10:55:33+01:00",
    "input_file": "vroom-vs-readr.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-07-my-first-post/",
    "title": "Building a Chat Analyzer app - Part 1",
    "description": "A workflow on how I plan to build this project",
    "author": [
      {
        "name": "Emmanuel Ugochukwu",
        "url": "https://github.com/emmanuelugo"
      }
    ],
    "date": "2022-02-07",
    "categories": [
      "NLP",
      "Shiny",
      "Text Analytics",
      "WhatsApp series"
    ],
    "contents": "\r\n\r\nContents\r\nFeatures\r\nChat Summary Feature\r\nWordcloud and Sentiment Analysis Feature\r\nWord Association Feature\r\nComparison Feature\r\nExtras\r\n\r\n\r\nHello üëã\r\nIn this post, I go over my plans and ideas on a new project I am embarking on, A Whatsapp Chat Analyzer. It is a fun app where the user uploads their chat history and they are filled with fun fact information about their chats.\r\nThe number of features that would be in this app hasn‚Äôt been decided yet but with what I have currently, it should be a fun app. I did a little social experiment with friends and found out how interesting this project would be. Simply telling friends that they have sent X amount of messages over a particular period or they use a particular word all the time or some little fun fact can put a smile on their face. In turn, this project would solidify my understanding on Text Analytics and improve my R Shiny skills.\r\nLet‚Äôs go over some of the features of the app, shall we?\r\nFeatures\r\nChat Summary Feature\r\nThis answers the following questions;\r\nTotal number of messages sent.\r\nTotal number of users.\r\nMost active user (by number of text sent)\r\nUser with the highest average length of text.\r\nThe least active user (by number of text sent).\r\nThe user who uses a lot of Emojis üòÖ\r\nPlots that would be generated in here would include a plot of the (20) most active users by number of text sent and a plot of the top users with highest average text length.\r\nWordcloud and Sentiment Analysis Feature\r\nHere, I plan on grouping text into Happy and Sad sentiments and displaying them with wordclouds and also perform a sentiment trend analysis since there would be a date column.\r\nWord Association Feature\r\nThis would group words together so we can understand how some words are connected. A network graph would be used to display the result.\r\nComparison Feature\r\nThis feature would drill down to a particular person or group of people and would highlight the following:\r\nUnique words for each user(s).\r\nActivity Status: Average chat length, Number of messages sent, Ranking activity level by some time variables.\r\nWordcloud to display most used words.\r\nLexical Diversity.\r\nSentiments.\r\nA feature to download these analysis.\r\nExtras\r\nAnother fun features to implement would be:\r\nWhen a user enters a word, the app returns a plot of the people who has used that word the most and also a popularity trend and the total number of times that word was used.\r\nWith these few ideas, the app should serve as a good data science project. I gave myself a deadline of 2 months to complete the app and I will try my best to beat that time.\r\nI will be writing more blogs covering the entire process.\r\nSee you soon!\r\n‚ù§Ô∏è\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-07-my-first-post/text-analytics.png",
    "last_modified": "2022-02-19T00:35:19+01:00",
    "input_file": "my-first-post.knit.md",
    "preview_width": 646,
    "preview_height": 569
  }
]
